{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XpNMS7Vk6Df"
      },
      "source": [
        "# HW4P2: Attention-based Speech Recognition\n",
        "\n",
        "<img src=\"https://cdn.shopify.com/s/files/1/0272/2080/3722/products/SmileBumperSticker_5400x.jpg\" alt=\"A cute cat\" width=\"600\">\n",
        "\n",
        "\n",
        "Welcome to the final assignment in 11785. In this HW, you will work on building a speech recognition system with <i>attention</i>. <br> <br>\n",
        "\n",
        "<center>\n",
        "<img src=\"https://popmn.org/wp-content/uploads/2020/03/pay-attention.jpg\" alt=\"A cute cat\" height=\"100\">\n",
        "</center>\n",
        "\n",
        "HW Writeup: [TODO] <br>\n",
        "Kaggle Competition Link: https://www.kaggle.com/competitions/attention-based-speech-recognition <br>\n",
        "Kaggle Dataset Link: https://www.kaggle.com/competitions/attention-based-speech-recognition/data\n",
        "<br>\n",
        "LAS Paper: https://arxiv.org/pdf/1508.01211.pdf <br>\n",
        "Attention is all you need:https://arxiv.org/pdf/1706.03762.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwIdDTTmmZVe"
      },
      "source": [
        "# Read this section importantly!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9qsVrRemgh7"
      },
      "source": [
        "1. By now, we believe that you are already a great deep learning practitioner, Congratulations. 🎉\n",
        "\n",
        "2. You are allowed to use code from your previous homeworks for this homework. We will only provide, aspects that are necessary and new with this homework.\n",
        "\n",
        "3. There are a lot of resources provided in this notebook, that will help you check if you are running your implementations correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UK7J-dp5iN5",
        "outputId": "66802957-853d-4169-8f19-d142f9a9e68f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu Sep  4 10:17:46 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 570.153                Driver Version: 573.26         CUDA Version: 12.8     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA GeForce RTX 5060 ...    On  |   00000000:01:00.0  On |                  N/A |\n",
            "| N/A   46C    P8             12W /   65W |     286MiB /   8151MiB |     22%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nYgaLmgy5iqR"
      },
      "outputs": [],
      "source": [
        "# Install some required libraries\n",
        "# Feel free to add more if you want\n",
        "!pip install -q python-levenshtein torchsummaryX wandb kaggle pytorch-nlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mkii-6Dsjr8",
        "outputId": "c2350613-0873-4c15-e4cc-657e6cf3fa0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from torch import nn, Tensor\n",
        "# import torchsummary\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import gc\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm as blue_tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "import math\n",
        "import random\n",
        "from typing import Optional, List\n",
        "\n",
        "#imports for decoding and distance calculation\n",
        "try:\n",
        "    import wandb\n",
        "    import torchsummaryX\n",
        "    import Levenshtein\n",
        "except:\n",
        "    print(\"Didnt install some/all imports\")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIOBPQjzrx5n"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EPchlig7rxia"
      },
      "outputs": [],
      "source": [
        "config = dict (\n",
        "    train_dataset       = 'train-clean-360', # train-clean-100, train-clean-360, train-clean-460\n",
        "    batch_size          = 96,\n",
        "    epochs              = 100,\n",
        "    learning_rate       = 2e-4,\n",
        "    weight_decay        = 5e-3,\n",
        "    cepstral_norm            = True, # Whether to use MFCC features or Spectrogram\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-njBvl2Opd6I"
      },
      "source": [
        "# Kaggle Dataset Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTyWR2sIp0Ns"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
        "!mkdir /root/.kaggle\n",
        "\n",
        "with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
        "    f.write('') # Put your kaggle username & key here\n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "F581gjfnqE2C"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n",
            "unzip:  cannot find or open attention-based-speech-recognition.zip, attention-based-speech-recognition.zip.zip or attention-based-speech-recognition.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "# # to download the dataset\n",
        "!kaggle competitions download -c attention-based-speech-recognition\n",
        "\n",
        "# # to unzip data quickly and quietly\n",
        "!unzip -q attention-based-speech-recognition.zip -d ./data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUJyBBwIqQs6"
      },
      "source": [
        "# Character-based LibriSpeech (HW4P2)\n",
        "\n",
        "In terms of the dataset, the dataset structure for HW3P2 and HW4P2 dataset are very similar. Can you spot out the differences? What all will be required??\n",
        "\n",
        "Hints:\n",
        "\n",
        "- Check how big is the dataset (do you require memory efficient loading techniques??)\n",
        "- How do we load mfccs? Do we need to normalise them?\n",
        "- Does the data have \\<SOS> and \\<EOS> tokens in each sequences? Do we remove them or do we not remove them? (Read writeup)\n",
        "- Would we want a collating function? Ask yourself: Why did we need a collate function last time?\n",
        "- Observe the VOCAB, is the dataset same as HW3P2?\n",
        "- Should you add augmentations, if yes which augmentations? When should you add augmentations? (Check bootcamp for answer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MBMLGYX-kZcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of vocab : 31\n",
            "Vocab           : ['<pad>', '<sos>', '<eos>', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', \"'\", ' ']\n",
            "PAD_TOKEN       : 0\n",
            "SOS_TOKEN       : 1\n",
            "EOS_TOKEN       : 2\n"
          ]
        }
      ],
      "source": [
        "VOCAB = [\n",
        "    '<pad>', '<sos>', '<eos>',\n",
        "    'A',   'B',    'C',    'D',\n",
        "    'E',   'F',    'G',    'H',\n",
        "    'I',   'J',    'K',    'L',\n",
        "    'M',   'N',    'O',    'P',\n",
        "    'Q',   'R',    'S',    'T',\n",
        "    'U',   'V',    'W',    'X',\n",
        "    'Y',   'Z',    \"'\",    ' ',\n",
        "]\n",
        "\n",
        "VOCAB_MAP = {VOCAB[i]:i for i in range(0, len(VOCAB))}\n",
        "\n",
        "PAD_TOKEN = VOCAB_MAP[\"<pad>\"]\n",
        "SOS_TOKEN = VOCAB_MAP[\"<sos>\"]\n",
        "EOS_TOKEN = VOCAB_MAP[\"<eos>\"]\n",
        "\n",
        "print(f\"Length of vocab : {len(VOCAB)}\")\n",
        "print(f\"Vocab           : {VOCAB}\")\n",
        "print(f\"PAD_TOKEN       : {PAD_TOKEN}\")\n",
        "print(f\"SOS_TOKEN       : {SOS_TOKEN}\")\n",
        "print(f\"EOS_TOKEN       : {EOS_TOKEN}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VuneWaTStdF2"
      },
      "outputs": [],
      "source": [
        "class SpeechDataset(torch.utils.data.Dataset): # Memory efficient\n",
        "    # Loades the data in get item to save RAM\n",
        "\n",
        "    def __init__(self, root, partition= \"train-clean-360\", transforms = None, cepstral=True):\n",
        "\n",
        "        self.VOCAB      = VOCAB\n",
        "        self.cepstral   = cepstral\n",
        "\n",
        "        if partition == \"train-clean-100\" or partition == \"train-clean-360\":\n",
        "            mfcc_dir       = os.path.join(root, partition, \"mfcc\")\n",
        "            transcript_dir = os.path.join(root, partition, \"transcripts\")\n",
        "\n",
        "            mfcc_files          = [os.path.join(mfcc_dir, f) for f in os.listdir(mfcc_dir)]\n",
        "            transcript_files    = [os.path.join(transcript_dir, f) for f in os.listdir(transcript_dir)]\n",
        "\n",
        "        else:\n",
        "            mfcc_dir       = os.path.join(root, \"train-clean-100\", \"mfcc\")\n",
        "            transcript_dir = os.path.join(root, \"train-clean-100\", \"transcripts\")\n",
        "\n",
        "            mfcc_files          = [os.path.join(mfcc_dir, f) for f in os.listdir(mfcc_dir)]\n",
        "            transcript_files    = [os.path.join(transcript_dir, f) for f in os.listdir(transcript_dir)]\n",
        "\n",
        "            mfcc_dir       = os.path.join(root, \"train-clean-360\", \"mfcc\")\n",
        "            transcript_dir = os.path.join(root, \"train-clean-360\", \"transcripts\")\n",
        "\n",
        "            # add the list of mfcc and transcript paths from train-clean-360 to the list of paths  from train-clean-100\n",
        "            mfcc_files.extend([os.path.join(mfcc_dir, f) for f in os.listdir(mfcc_dir)])\n",
        "            transcript_files.extend([os.path.join(transcript_dir, f) for f in os.listdir(transcript_dir)])\n",
        "\n",
        "        assert len(mfcc_files) == len(transcript_files)\n",
        "        length = len(mfcc_files)\n",
        "\n",
        "        self.mfcc_files         = mfcc_files\n",
        "        self.transcript_files   = transcript_files\n",
        "        self.length             = len(transcript_files)\n",
        "        print(\"Loaded file paths ME: \", partition)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "\n",
        "        # Load the mfcc and transcripts from the mfcc and transcript paths created earlier\n",
        "        mfcc        = np.load(self.mfcc_files[ind])\n",
        "        transcript  = np.load(self.transcript_files[ind])\n",
        "        # Normalize the mfccs and map the transcripts to integers\n",
        "        mfcc                = (mfcc - np.mean(mfcc)) / np.std(mfcc)\n",
        "        transcript_mapped   = [VOCAB_MAP[c] for c in transcript]\n",
        "\n",
        "        return torch.FloatTensor(mfcc), torch.LongTensor(transcript_mapped)\n",
        "\n",
        "    def collate_fn(self,batch):\n",
        "\n",
        "        batch_x, batch_y, lengths_x, lengths_y = [], [], [], []\n",
        "\n",
        "        for x, y in batch:\n",
        "            batch_x.append(x)\n",
        "            batch_y.append(y)\n",
        "\n",
        "            # Add the mfcc, transcripts and their lengths to the lists created above\n",
        "            lengths_x.append(x.size(0))\n",
        "            lengths_y.append(y.size(0))\n",
        "\n",
        "        # pack the mfccs and transcripts using the pad_sequence function from pytorch\n",
        "        batch_x_pad = nn.utils.rnn.pad_sequence(batch_x, batch_first=True)\n",
        "        batch_y_pad = nn.utils.rnn.pad_sequence(batch_y, batch_first=True)\n",
        "\n",
        "        return batch_x_pad, batch_y_pad, torch.tensor(lengths_x), torch.tensor(lengths_y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jUrqTkG4VZfJ"
      },
      "outputs": [],
      "source": [
        "class SpeechDatasetTest(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, root, partition, cepstral=False):\n",
        "        from tqdm import tqdm\n",
        "        self.mfcc_dir   = os.path.join(root, partition, \"mfcc\") # path to the test-clean mfccs\n",
        "        self.mfcc_files = [os.path.join(self.mfcc_dir, f) for f in os.listdir(self.mfcc_dir)]\n",
        "\n",
        "        self.mfccs = []\n",
        "        for i, filename in enumerate(tqdm(self.mfcc_files)):\n",
        "            mfcc = np.load(filename)\n",
        "            if cepstral:\n",
        "                # Normalize the mfccs\n",
        "                mfcc = (mfcc - np.mean(mfcc)) / np.std(mfcc)\n",
        "            # append the mfcc to the mfcc list created earlier\n",
        "            self.mfccs.append(mfcc)\n",
        "\n",
        "\n",
        "        print(\"Loaded: \", partition)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.mfccs)\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        mfcc = self.mfccs[ind]\n",
        "        return torch.FloatTensor(mfcc)\n",
        "\n",
        "    def collate_fn(self,batch):\n",
        "\n",
        "        batch_x, lengths_x = [], []\n",
        "        for x in batch:\n",
        "            # Append the mfccs and their lengths to the lists created above\n",
        "            batch_x.append(x)\n",
        "            lengths_x.append(x.size(0))\n",
        "        # pack the mfccs using the pad_sequence function from pytorch\n",
        "        batch_x_pad = nn.utils.rnn.pad_sequence(batch_x, batch_first=True)\n",
        "\n",
        "        return batch_x_pad, torch.tensor(lengths_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rsl5Q1jLvsOL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded file paths ME:  train-clean-360\n",
            "Loaded file paths ME:  dev-clean\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2620/2620 [00:01<00:00, 2298.68it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded:  test-clean\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "253"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "DATA_DIR        = 'data'\n",
        "PARTITION       = config['train_dataset']\n",
        "CEPSTRAL        = config['cepstral_norm']\n",
        "\n",
        "train_dataset   = SpeechDataset( # Or AudioDatasetME\n",
        "    root        = DATA_DIR,\n",
        "    partition   = PARTITION,\n",
        "    cepstral    = CEPSTRAL\n",
        ")\n",
        "valid_dataset   = SpeechDataset(\n",
        "    root        = DATA_DIR,\n",
        "    partition   = 'dev-clean',\n",
        "    cepstral    = CEPSTRAL\n",
        ")\n",
        "test_dataset    = SpeechDatasetTest(\n",
        "    root        = DATA_DIR,\n",
        "    partition   = 'test-clean',\n",
        "    cepstral    = CEPSTRAL,\n",
        ")\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OeqXHogpwFfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No. of train mfccs   :  104013\n",
            "Batch size           :  96\n",
            "Train batches        :  1084\n",
            "Valid batches        :  1381\n",
            "Test batches         :  28\n"
          ]
        }
      ],
      "source": [
        "train_loader    = torch.utils.data.DataLoader(\n",
        "    dataset     = train_dataset,\n",
        "    batch_size  = config['batch_size'],\n",
        "    shuffle     = True,\n",
        "    num_workers = 0,\n",
        "    pin_memory  = True,\n",
        "    collate_fn  = train_dataset.collate_fn\n",
        ")\n",
        "\n",
        "valid_loader    = torch.utils.data.DataLoader(\n",
        "    dataset     = valid_dataset,\n",
        "    batch_size  = config['batch_size'],\n",
        "    shuffle     = False,\n",
        "    num_workers = 0,\n",
        "    pin_memory  = True,\n",
        "    collate_fn  = valid_dataset.collate_fn\n",
        ")\n",
        "\n",
        "test_loader     = torch.utils.data.DataLoader(\n",
        "    dataset     = test_dataset,\n",
        "    batch_size  = config['batch_size'],\n",
        "    shuffle     = False,\n",
        "    num_workers = 0,\n",
        "    pin_memory  = True,\n",
        "    collate_fn  = test_dataset.collate_fn\n",
        ")\n",
        "\n",
        "print(\"No. of train mfccs   : \", train_dataset.__len__())\n",
        "print(\"Batch size           : \", config['batch_size'])\n",
        "print(\"Train batches        : \", train_loader.__len__())\n",
        "print(\"Valid batches        : \", valid_loader.__len__())\n",
        "print(\"Test batches         : \", test_loader.__len__())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzuIXCyAuNvo",
        "outputId": "633dcb9a-d1ef-46f2-a896-35e31f97543a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Checking the shapes of the data...\n",
            "torch.Size([96, 1635, 28]) torch.Size([96, 292]) torch.Size([96]) torch.Size([96])\n",
            "tensor([[ 1, 16,  7,  ...,  0,  0,  0],\n",
            "        [ 1, 21,  7,  ...,  0,  0,  0],\n",
            "        [ 1, 21, 11,  ...,  0,  0,  0],\n",
            "        ...,\n",
            "        [ 1, 25,  3,  ...,  0,  0,  0],\n",
            "        [ 1,  3, 16,  ...,  0,  0,  0],\n",
            "        [ 1, 21, 22,  ...,  0,  0,  0]])\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nChecking the shapes of the data...\")\n",
        "for batch in train_loader:\n",
        "    x, y, x_len, y_len = batch\n",
        "    print(x.shape, y.shape, x_len.shape, y_len.shape)\n",
        "    print(y)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Checking the shapes of the data...\n",
            "torch.Size([96, 1616, 28]) torch.Size([96, 264]) torch.Size([96]) torch.Size([96])\n",
            "tensor([[ 1,  3, 16,  ...,  0,  0,  0],\n",
            "        [ 1, 25, 10,  ...,  0,  0,  0],\n",
            "        [ 1, 22, 10,  ...,  0,  0,  0],\n",
            "        ...,\n",
            "        [ 1, 22, 10,  ...,  0,  0,  0],\n",
            "        [ 1, 10, 11,  ...,  0,  0,  0],\n",
            "        [ 1,  3, 30,  ...,  0,  0,  0]])\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nChecking the shapes of the data...\")\n",
        "for batch in train_loader:\n",
        "    x, y, x_len, y_len = batch\n",
        "    print(x.shape, y.shape, x_len.shape, y_len.shape)\n",
        "    print(y)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "8QFdYrM7xcI1",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Partition loaded     :  train-clean-100\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[60], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax mfcc length          : \u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mmax([data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m dataset]))\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvg mfcc length          : \u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mmean([data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m dataset]))\n\u001b[0;32m---> 12\u001b[0m \u001b[43mverify_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain-clean-100\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m verify_dataset(valid_dataset, partition\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdev-clean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m verify_dataset(test_dataset, partition\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest-clean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "Cell \u001b[0;32mIn[60], line 4\u001b[0m, in \u001b[0;36mverify_dataset\u001b[0;34m(dataset, partition)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPartition loaded     : \u001b[39m\u001b[38;5;124m\"\u001b[39m, partition)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m partition \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest-clean\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax mfcc length          : \u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mmax([data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m dataset]))\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvg mfcc length          : \u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mmean([data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m dataset]))\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax transcript length    : \u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mmax([data[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m dataset]))\n",
            "Cell \u001b[0;32mIn[60], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPartition loaded     : \u001b[39m\u001b[38;5;124m\"\u001b[39m, partition)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m partition \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest-clean\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax mfcc length          : \u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mmax([data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m dataset]))\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvg mfcc length          : \u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mmean([data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m dataset]))\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax transcript length    : \u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mmax([data[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m dataset]))\n",
            "Cell \u001b[0;32mIn[56], line 46\u001b[0m, in \u001b[0;36mSpeechDataset.__getitem__\u001b[0;34m(self, ind)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, ind):\n\u001b[1;32m     43\u001b[0m \n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# Load the mfcc and transcripts from the mfcc and transcript paths created earlier\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     mfcc        \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmfcc_files[ind])\n\u001b[0;32m---> 46\u001b[0m     transcript  \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranscript_files\u001b[49m\u001b[43m[\u001b[49m\u001b[43mind\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# Normalize the mfccs and map the transcripts to integers\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     mfcc                \u001b[38;5;241m=\u001b[39m (mfcc \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(mfcc)) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(mfcc)\n",
            "File \u001b[0;32m~/miniconda3/envs/zonos/lib/python3.10/site-packages/numpy/lib/_npyio_impl.py:458\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    456\u001b[0m _ZIP_SUFFIX \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPK\u001b[39m\u001b[38;5;130;01m\\x05\u001b[39;00m\u001b[38;5;130;01m\\x06\u001b[39;00m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# empty zip files start with this\u001b[39;00m\n\u001b[1;32m    457\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mMAGIC_PREFIX)\n\u001b[0;32m--> 458\u001b[0m magic \u001b[38;5;241m=\u001b[39m \u001b[43mfid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m magic:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo data left in file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def verify_dataset(dataset, partition= 'train-clean-100'):\n",
        "    print(\"\\nPartition loaded     : \", partition)\n",
        "    if partition != 'test-clean':\n",
        "        print(\"Max mfcc length          : \", np.max([data[0].shape[0] for data in dataset]))\n",
        "        print(\"Avg mfcc length          : \", np.mean([data[0].shape[0] for data in dataset]))\n",
        "        print(\"Max transcript length    : \", np.max([data[1].shape[0] for data in dataset]))\n",
        "        print(\"Max transcript length    : \", np.mean([data[1].shape[0] for data in dataset]))\n",
        "    else:\n",
        "        print(\"Max mfcc length          : \", np.max([data.shape[0] for data in dataset]))\n",
        "        print(\"Avg mfcc length          : \", np.mean([data.shape[0] for data in dataset]))\n",
        "\n",
        "verify_dataset(train_dataset, partition= 'train-clean-100')\n",
        "verify_dataset(valid_dataset, partition= 'dev-clean')\n",
        "verify_dataset(test_dataset, partition= 'test-clean')\n",
        "dataset_max_len  = max(\n",
        "    np.max([data[0].shape[0] for data in train_dataset]),\n",
        "    np.max([data[0].shape[0] for data in valid_dataset]),\n",
        "    np.max([data.shape[0] for data in test_dataset])\n",
        ")\n",
        "print(\"\\nMax Length: \", dataset_max_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_n3pqt7ud4t"
      },
      "source": [
        "Check if you are loading the data correctly with the following:\n",
        "\n",
        "- Train Dataset\n",
        "```\n",
        "Partition loaded:  train-clean-100\n",
        "Max mfcc length:  2448\n",
        "Average mfcc length:  1264.6258453344547\n",
        "Max transcript:  400\n",
        "Average transcript length:  186.65321139493324\n",
        "```\n",
        "\n",
        "- Dev Dataset\n",
        "```\n",
        "Partition loaded:  dev-clean\n",
        "Max mfcc length:  3260\n",
        "Average mfcc length:  713.3570107288198\n",
        "Max transcript:  518\n",
        "Average transcript length:  108.71698113207547\n",
        "```\n",
        "\n",
        "- Test Dataset\n",
        "```\n",
        "Partition loaded:  test-clean\n",
        "Max mfcc length:  3491\n",
        "Average mfcc length:  738.2206106870229\n",
        "```\n",
        "\n",
        "If your values is not matching, read hints, think what could have gone wrong. Then approach TAs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8q9wt4TwzPt"
      },
      "source": [
        "# THE MODEL\n",
        "\n",
        "### Listen, Attend and Spell\n",
        "Listen, Attend and Spell (LAS) is a neural network model used for speech recognition and synthesis tasks.\n",
        "\n",
        "- LAS is designed to handle long input sequences and is robust to noisy speech signals.\n",
        "- LAS is known for its high accuracy and ability to improve over time with additional training data.\n",
        "- It consists of an <b>listener, an attender and a speller</b>, which work together to convert an input speech signal into a corresponding output text.\n",
        "\n",
        "#### The Dataflow:\n",
        "<center>\n",
        "<img src=\"https://github.com/varunjain3/11785_s23_h4p2/raw/main/DataFlow.png\" alt=\"data flow\" height=\"100\">\n",
        "</center>\n",
        "\n",
        "#### The Listener:\n",
        "- converts the input speech signal into a sequence of hidden states.\n",
        "\n",
        "#### The Attender:\n",
        "- Decides how the sequence of Encoder hidden state is propogated to decoder.\n",
        "\n",
        "#### The Speller:\n",
        "- A language model, that incorporates the \"context of attender\"(output of attender) to predict sequence of words.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTZ-lv47XOj0"
      },
      "source": [
        "## Utils\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FuRzPOaX0EtJ"
      },
      "outputs": [],
      "source": [
        "class PermuteBlock(torch.nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.transpose(1, 2)\n",
        "\n",
        "def plot_attention(attention):\n",
        "    # Function for plotting attention\n",
        "    # You need to get a diagonal plot\n",
        "    plt.clf()\n",
        "    seaborn.heatmap(attention, cmap='GnBu')\n",
        "    plt.show()\n",
        "\n",
        "def save_model(model, optimizer, scheduler, tf_scheduler, metric, epoch, path):\n",
        "    torch.save(\n",
        "        {'model_state_dict'         : model.state_dict(),\n",
        "         'optimizer_state_dict'     : optimizer.state_dict(),\n",
        "         'scheduler_state_dict'     : scheduler.state_dict(),\n",
        "         'tf_scheduler'             : tf_scheduler,\n",
        "         metric[0]                  : metric[1],\n",
        "         'epoch'                    : epoch},\n",
        "         path\n",
        "    )\n",
        "\n",
        "def load_model(best_path, epoch_path, model, mode= 'best', metric= 'valid_acc', optimizer= None, scheduler= None, tf_scheduler= None):\n",
        "\n",
        "\n",
        "    if mode == 'best':\n",
        "        checkpoint  = torch.load(best_path)\n",
        "        print(\"Loading best checkpoint: \", checkpoint[metric])\n",
        "    else:\n",
        "        checkpoint  = torch.load(epoch_path)\n",
        "        print(\"Loading epoch checkpoint: \", checkpoint[metric])\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'], strict= False)\n",
        "\n",
        "    if optimizer != None:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        #optimizer.param_groups[0]['lr'] = 1.5e-3\n",
        "        optimizer.param_groups[0]['weight_decay'] = 1e-5\n",
        "    if scheduler != None:\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "    if tf_scheduler != None:\n",
        "        tf_scheduler    = checkpoint['tf_scheduler']\n",
        "\n",
        "    epoch   = checkpoint['epoch']\n",
        "    metric  = torch.load(best_path)[metric]\n",
        "\n",
        "    return [model, optimizer, scheduler, tf_scheduler, epoch, metric]\n",
        "\n",
        "class TimeElapsed():\n",
        "    def __init__(self):\n",
        "        self.start  = -1\n",
        "\n",
        "    def time_elapsed(self):\n",
        "        if self.start == -1:\n",
        "            self.start = time.time()\n",
        "        else:\n",
        "            end = time.time() - self.start\n",
        "            hrs, rem    = divmod(end, 3600)\n",
        "            min, sec    = divmod(rem, 60)\n",
        "            min         = min + 60*hrs\n",
        "            print(\"Time Elapsed: {:0>2}:{:02}\".format(int(min),int(sec)))\n",
        "            self.start  = -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiUnK0GMXTY6"
      },
      "source": [
        "## Modules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUQUwEHmCxeI"
      },
      "source": [
        "# Transformer Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pLVW4Qw1C06t",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TransformerEncoder(\n",
            "  (KW): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (VW): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (QW): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (permute): PermuteBlock()\n",
            "  (attention): MultiheadAttention(\n",
            "    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "  )\n",
            "  (bn1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "  (bn2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "  (MLP): Sequential(\n",
            "    (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=128, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "class PositionalEncoding(torch.nn.Module):\n",
        "\n",
        "    # def __init__(self, projection_size, max_seq_len= 176):\n",
        "    def __init__(self, projection_size, max_seq_len= 1760):\n",
        "        super().__init__()\n",
        "        # Read the Attention Is All You Need paper to learn how to code code the positional encoding\n",
        "        pe = torch.zeros(max_seq_len, projection_size)\n",
        "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, projection_size, 2).float() * (-math.log(10000.0) / projection_size))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape [96, 108, 128]\n",
        "        x = x + self.pe[:x.size(1), :]\n",
        "        return x\n",
        "\n",
        "class TransformerEncoder(torch.nn.Module):\n",
        "    def __init__(self, projection_size, num_heads=8, dropout= 0.0):\n",
        "        super().__init__()\n",
        "\n",
        "        # create the key, query and value weights\n",
        "        self.KW         = nn.Linear(projection_size, projection_size)\n",
        "        self.VW         = nn.Linear(projection_size, projection_size)\n",
        "        self.QW         = nn.Linear(projection_size, projection_size)\n",
        "\n",
        "        self.permute    = PermuteBlock()\n",
        "\n",
        "        # Compute multihead attention. You are free to use the version provided by pytorch\n",
        "        self.attention  = nn.MultiheadAttention(embed_dim=projection_size, num_heads=num_heads, dropout=dropout, batch_first=True)\n",
        "\n",
        "        self.bn1        = nn.LayerNorm(projection_size)\n",
        "\n",
        "        self.bn2        = nn.LayerNorm(projection_size)\n",
        "\n",
        "        # Feed forward neural network\n",
        "        self.MLP        = nn.Sequential(\n",
        "            nn.Linear(projection_size, projection_size * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(projection_size * 4, projection_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # compute the key, query and value\n",
        "        key     = self.KW(x)\n",
        "        value   = self.VW(x)\n",
        "        query   = self.QW(x)\n",
        "\n",
        "        # compute the output of the attention module\n",
        "        out1    = self.attention(query, key, value, need_weights=False)\n",
        "        # Create a residual connection between the input and the output of the attention module\n",
        "        out1    = out1[0] + x\n",
        "        # Apply batch norm to out1\n",
        "        out1    = self.bn1(out1)\n",
        "\n",
        "        # Apply the output of the feed forward network\n",
        "        out2    = self.MLP(out1)\n",
        "        # Apply a residual connection between the input and output of the  FFN\n",
        "        out2    = out2 + out1 # could be x\n",
        "        # Apply batch norm to the output\n",
        "        out2    = self.bn2(out2)\n",
        "\n",
        "        return out2\n",
        "\n",
        "model   = TransformerEncoder(\n",
        "    projection_size  = 128\n",
        ").to(DEVICE)\n",
        "\n",
        "\n",
        "print(model)\n",
        "\n",
        "x_sample    = torch.rand(32, 176, 128)\n",
        "output      = model(x_sample.to(DEVICE))\n",
        "# torchsummaryX.summary(model, x_sample.to(DEVICE))\n",
        "del x_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "F0opYqry_EGi"
      },
      "outputs": [],
      "source": [
        "class TransformerListener(torch.nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 input_size,\n",
        "                 base_lstm_layers        = 1,\n",
        "                 pblstm_layers           = 1,\n",
        "                 listener_hidden_size    = 256,\n",
        "                 n_heads                 = 8,\n",
        "                 tf_blocks               = 1):\n",
        "        super().__init__()\n",
        "\n",
        "        # create an lstm layer\n",
        "        self.base_lstm      = nn.LSTM(input_size, listener_hidden_size//2, num_layers=base_lstm_layers, batch_first=True, bidirectional=True)\n",
        "\n",
        "        # create a sequence of Conv1d layers\n",
        "        # mind the paddings, mind the paddings, mind the paddings!!!\n",
        "        self.embedding      = nn.Sequential(\n",
        "            PermuteBlock(),\n",
        "            nn.Conv1d(listener_hidden_size, listener_hidden_size, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(listener_hidden_size, listener_hidden_size, kernel_size=5, stride=2, padding=2),\n",
        "            nn.ReLU(),\n",
        "            PermuteBlock()\n",
        "        )\n",
        "\n",
        "        # compute the postion encoding\n",
        "        self.positional_encoding    = PositionalEncoding(listener_hidden_size)\n",
        "\n",
        "        # create a sequence of transformer blocks\n",
        "        self.transformer_encoder    = torch.nn.Sequential()\n",
        "        for i in range(tf_blocks):\n",
        "            self.transformer_encoder.append(TransformerEncoder(listener_hidden_size, num_heads=n_heads))\n",
        "            \n",
        "\n",
        "    def forward(self, x, x_len):\n",
        "        # pack the inputs before passing them to the LSTm\n",
        "        x_packed                = nn.utils.rnn.pack_padded_sequence(x, x_len, batch_first=True, enforce_sorted=False)\n",
        "        # Pass the packed sequence through the lstm\n",
        "        lstm_out, _             = self.base_lstm(x_packed)\n",
        "        # Unpack the output of the lstm\n",
        "        output, output_lengths  = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
        "        \n",
        "        # Pass the output through the embedding\n",
        "        output                  = self.embedding(output)\n",
        "        # calculate the new output length\n",
        "        output_lengths          = (output_lengths + 1) // 2\n",
        "\n",
        "        # calculate the position encoding\n",
        "        output  = self.positional_encoding(output)\n",
        "        # Pass the output of the positional encoding through the transformer encoder\n",
        "        output  = self.transformer_encoder(output)\n",
        "\n",
        "\n",
        "        return output, output_lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 32, 11])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "listener_hidden_size = 32\n",
        "layers = nn.Sequential(\n",
        "            nn.Conv1d(listener_hidden_size, listener_hidden_size, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(listener_hidden_size, listener_hidden_size, kernel_size=5, stride=2, padding=2),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "layers = layers.to(DEVICE)\n",
        "x = torch.randn(4, 32, 21).to(DEVICE)\n",
        "layers(x).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "listener = TransformerListener(\n",
        "    input_size = 28,\n",
        "    base_lstm_layers=1,\n",
        "    listener_hidden_size=256,\n",
        "    n_heads = 8,\n",
        "    tf_blocks = 2,\n",
        ").to(DEVICE)\n",
        "\n",
        "for batch in train_loader:\n",
        "    x, y, x_len, y_len = batch\n",
        "    output = listener(x.to(DEVICE), x_len)\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fG9jDZBVklL"
      },
      "source": [
        "# Attention\n",
        "\n",
        "### Different ways to compute Attention\n",
        "\n",
        "1. Dot-product attention\n",
        "    * raw_weights = bmm(key, query)\n",
        "    * Optional: Scaled dot-product by normalizing with sqrt key dimension\n",
        "    * Check \"Attention is All You Need\" Section 3.2.1\n",
        "    * 1st way is what most TAs are comfortable with, but if you want to explore, check out other methods below\n",
        "\n",
        "\n",
        "2. Cosine attention\n",
        "    * raw_weights = cosine(query, key) # almost the same as dot-product xD\n",
        "\n",
        "3. Bi-linear attention\n",
        "    * W = Linear transformation (learnable parameter): d_k -> d_q\n",
        "    * raw_weights = bmm(key @ W, query)\n",
        "\n",
        "4. Multi-layer perceptron\n",
        "    * Check \"Neural Machine Translation and Sequence-to-sequence Models: A Tutorial\" Section 8.4\n",
        "\n",
        "5. Multi-Head Attention\n",
        "    * Check \"Attention is All You Need\" Section 3.2.2\n",
        "    * h = Number of heads\n",
        "    * W_Q, W_K, W_V: Weight matrix for Q, K, V (h of them in total)\n",
        "    * W_O: d_v -> d_v\n",
        "    * Reshape K: (B, T, d_k) to (B, T, h, d_k // h) and transpose to (B, h, T, d_k // h)\n",
        "    * Reshape V: (B, T, d_v) to (B, T, h, d_v // h) and transpose to (B, h, T, d_v // h)\n",
        "    * Reshape Q: (B, d_q) to (B, h, d_q // h) `\n",
        "    * raw_weights = Q @ K^T\n",
        "    * masked_raw_weights = mask(raw_weights)\n",
        "    * attention = softmax(masked_raw_weights)\n",
        "    * multi_head = attention @ V\n",
        "    * multi_head = multi_head reshaped to (B, d_v)\n",
        "    * context = multi_head @ W_O"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyv0Q65t5SDd"
      },
      "source": [
        "Pseudocode:\n",
        "\n",
        "```python\n",
        "class Attention:\n",
        "    '''\n",
        "    Attention is calculated using the key, value (from encoder embeddings) and query from decoder.\n",
        "\n",
        "    After obtaining the raw weights, compute and return attention weights and context as follows.:\n",
        "\n",
        "    attention_weights   = softmax(raw_weights)\n",
        "    attention_context   = einsum(\"thinkwhatwouldbetheequationhere\",attention, value) #take hint from raw_weights calculation\n",
        "\n",
        "    At the end, you can pass context through a linear layer too.\n",
        "    '''\n",
        "\n",
        "    def init(listener_hidden_size,\n",
        "              speller_hidden_size,\n",
        "              projection_size):\n",
        "\n",
        "        VW = Linear(listener_hidden_size,projection_size)\n",
        "        KW = Linear(listener_hidden_size,projection_size)\n",
        "        QW = Linear(speller_hidden_size,projection_size)\n",
        "\n",
        "    def set_key_value(encoder_outputs):\n",
        "        '''\n",
        "        In this function we take the encoder embeddings and make key and values from it.\n",
        "        key.shape   = (batch_size, timesteps, projection_size)\n",
        "        value.shape = (batch_size, timesteps, projection_size)\n",
        "        '''\n",
        "        key = KW(encoder_outputs)\n",
        "        value = VW(encoder_outputs)\n",
        "      \n",
        "    def compute_context(decoder_context):\n",
        "        '''\n",
        "        In this function from decoder context, we make the query, and then we\n",
        "         multiply the queries with the keys to find the attention logits,\n",
        "         finally we take a softmax to calculate attention energy which gets\n",
        "         multiplied to the generted values and then gets summed.\n",
        "\n",
        "        key.shape   = (batch_size, timesteps, projection_size)\n",
        "        value.shape = (batch_size, timesteps, projection_size)\n",
        "        query.shape = (batch_size, projection_size)\n",
        "\n",
        "        You are also recomended to check out Abu's Lecture 19 to understand Attention better.\n",
        "        '''\n",
        "        query = QW(decoder_context) #(batch_size, projection_size)\n",
        "\n",
        "        raw_weights = #using bmm or einsum. We need to perform batch matrix multiplication. It is important you do this step correctly.\n",
        "        #What will be the shape of raw_weights?\n",
        "\n",
        "        attention_weights = #What makes raw_weights -> attention_weights\n",
        "\n",
        "        attention_context = #Multiply attention weights to values\n",
        "\n",
        "        return attention_context, attention_weights\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "771TXxn7ViOW"
      },
      "outputs": [],
      "source": [
        "from einops import einsum, rearrange\n",
        "\n",
        "class Attention(torch.nn.Module):\n",
        "  def __init__(self, listener_hidden_size, speller_hidden_size, projection_size, num_heads=8):\n",
        "    super().__init__()\n",
        "    self.QW = nn.Linear(speller_hidden_size, projection_size)\n",
        "    self.KW = nn.Linear(listener_hidden_size, projection_size)\n",
        "    self.VW = nn.Linear(listener_hidden_size, projection_size)\n",
        "    self.n_head = num_heads\n",
        "    \n",
        "  def set_key_value(self, encoder_outputs):\n",
        "    self.key = self.KW(encoder_outputs)\n",
        "    self.value = self.VW(encoder_outputs)\n",
        "\n",
        "  def compute_context(self, decoder_context):\n",
        "    query = self.QW(decoder_context)\n",
        "    query = rearrange(query, 'b (h d) -> b h d', h=self.n_head)\n",
        "    key = rearrange(self.key, 'b t (h d) -> b h t d', h=self.n_head)\n",
        "    value = rearrange(self.value, 'b t (h d) -> b h t d', h=self.n_head)\n",
        "    scale = math.sqrt(key.size(-1))\n",
        "    att = einsum(query, key, 'b h d, b h k d -> b h k') / scale\n",
        "    att = nn.functional.softmax(att, dim=-1)\n",
        "    context = einsum(att, value, 'b h t, b h t d -> b h d')\n",
        "    context = rearrange(context, 'b h d -> b (h d)') # re-assemble all head outputs side by side\n",
        "\n",
        "    return context, att"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Sp1WywZmm1L"
      },
      "source": [
        "# The Speller\n",
        "\n",
        "Similar to the language model that you coded up for HW4P1, you have to code a language model for HW4P2 as well. This time, we will also call the attention context step, within the decoder to get the attended-encoder-embeddings.\n",
        "\n",
        "\n",
        "What you have coded till now:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/varunjain3/11785_s23_h4p2/raw/main/EncoderAttention.png\" alt=\"data flow\" height=\"400\">\n",
        "</center>\n",
        "\n",
        "For the Speller, what we have to code:\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/varunjain3/11785_s23_h4p2/raw/main/Decoder.png\" alt=\"data flow\" height=\"400\">\n",
        "</center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "nFkc6MbnlUPu"
      },
      "outputs": [],
      "source": [
        "class Speller(torch.nn.Module):\n",
        "\n",
        "  # Refer to your HW4P1 implementation for help with setting up the language model.\n",
        "  # The only thing you need to implement on top of your HW4P1 model is the attention module and teacher forcing.\n",
        "\n",
        "  def __init__(self, attender:Attention, embedding_dim=128, hidden_dim=256, num_lstm_layers=2, context_dim=128):\n",
        "    super(). __init__()\n",
        "    assert hidden_dim == embedding_dim + context_dim, \"Hidden dim should be equal to embedding + context dim\"\n",
        "    # config\n",
        "    vocab_size          = len(VOCAB)\n",
        "    self.embedding_dim   = embedding_dim\n",
        "    self.hidden_dim      = hidden_dim\n",
        "    self.context_dim     = context_dim\n",
        "    self.num_lstm_layers = num_lstm_layers\n",
        "\n",
        "    self.attend = attender # Attention object in speller\n",
        "    self.max_timesteps = 176 # Max timesteps\n",
        "\n",
        "    self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
        "    self.lstm_cells = nn.ModuleList([\n",
        "      nn.LSTMCell(input_size=hidden_dim, hidden_size=hidden_dim) \n",
        "      for _ in range(num_lstm_layers)\n",
        "    ])\n",
        "\n",
        "    # For CDN (Feel free to change)\n",
        "    self.output_to_char = nn.Linear(hidden_dim + context_dim, embedding_dim) # Linear module to convert outputs to correct hidden size (Optional: TO make dimensions match)\n",
        "    self.activation = nn.ReLU() # Check which activation is suggested\n",
        "    self.char_prob = nn.Linear(embedding_dim, vocab_size)# Linear layer to convert hidden space back to logits for token classification\n",
        "    self.char_prob.weight = self.embedding.weight # Weight tying (From embedding layer)\n",
        "\n",
        "\n",
        "\n",
        "  def lstm_step(self, input_word, hidden_state):\n",
        "    lstm_input = input_word\n",
        "    for i in range(len(self.lstm_cells)):\n",
        "      hidden_state[i][0], hidden_state[i][1] = self.lstm_cells[i](lstm_input, (hidden_state[i][0], hidden_state[i][1])) # Feed the input through each LSTM Cell\n",
        "      lstm_input = hidden_state[i][0]\n",
        "    return hidden_state # What information does forward() need?\n",
        "\n",
        "  def CDN(self, x):\n",
        "    # Make the CDN here, you can add the output-to-char\n",
        "    x = self.output_to_char(x)\n",
        "    x = self.activation(x)\n",
        "    x = self.char_prob(x)\n",
        "    return x\n",
        "\n",
        "  def forward (self, y=None, teacher_forcing_ratio=1):\n",
        "\n",
        "    attn_context = torch.zeros((config[\"batch_size\"], self.context_dim,)).to(DEVICE) # initial context tensor for time t = 0\n",
        "    output_symbol = torch.LongTensor([SOS_TOKEN]*config[\"batch_size\"]).to(DEVICE) # Set it to SOS for time t = 0\n",
        "    raw_outputs = []\n",
        "    attention_plot = []\n",
        "\n",
        "    if y is None:\n",
        "      timesteps = self.max_timesteps\n",
        "      teacher_forcing_ratio = 0 #Why does it become zero?\n",
        "\n",
        "    else:\n",
        "      timesteps = y.size(1) # How many timesteps are we predicting for?\n",
        "\n",
        "    hidden_states_list = [[torch.zeros((config[\"batch_size\"], self.hidden_dim)).to(DEVICE), \n",
        "                           torch.zeros((config[\"batch_size\"], self.hidden_dim)).to(DEVICE)\n",
        "                          ] for _ in range(len(self.lstm_cells))] # Initialize your hidden_states list here similar to HW4P1\n",
        "\n",
        "    for t in tqdm(range(timesteps)):\n",
        "      p = random.uniform(0, 1)# generate a probability p between 0 and 1\n",
        "\n",
        "      if p < teacher_forcing_ratio and t > 0: # Why do we consider cases only when t > 0? What is considered when t == 0? Think.\n",
        "        output_symbol = y[:, t-1]# Take from y, else draw from probability distribution\n",
        "\n",
        "      char_embed = self.embedding(output_symbol) # Embed the character symbol\n",
        "\n",
        "      # Concatenate the character embedding and context from attention, as shown in the diagram\n",
        "      lstm_input = torch.cat([char_embed, attn_context], dim=-1)\n",
        "\n",
        "      hidden_states_list = self.lstm_step(lstm_input, hidden_states_list) # Feed the input through LSTM Cells and attention.\n",
        "      # What should we retrieve from forward_step to prepare for the next timestep?\n",
        "\n",
        "      attn_context, attn_weights = self.attend.compute_context(hidden_states_list[-1][0]) # Feed the resulting hidden state into attention\n",
        "\n",
        "      cdn_input = torch.cat([attn_context, hidden_states_list[-1][0]], dim=-1) # Concatenate context and LSTM output\n",
        "\n",
        "      raw_pred = self.CDN(cdn_input)\n",
        "\n",
        "      # Generate a prediction for this timestep and collect it in output_symbols\n",
        "      output_symbol = torch.argmax(raw_pred, dim=-1) # Draw correctly from raw_pred\n",
        "\n",
        "      raw_outputs.append(raw_pred) # for loss calculation\n",
        "      attention_plot.append(attn_weights) # for plotting attention plot\n",
        "\n",
        "      print(t)\n",
        "\n",
        "    attention_plot = torch.stack(attention_plot, dim=1)\n",
        "    raw_outputs = torch.stack(raw_outputs, dim=1)\n",
        "\n",
        "    return raw_outputs, attention_plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "scvB2cI-OSof"
      },
      "outputs": [],
      "source": [
        "class ASRModel(torch.nn.Module):\n",
        "  def __init__(self,): # add parameters\n",
        "    super().__init__()\n",
        "\n",
        "    # Pass the right parameters here\n",
        "    self.listener = TransformerListener(\n",
        "        input_size = 28,\n",
        "        base_lstm_layers=1,\n",
        "        listener_hidden_size=128,\n",
        "        n_heads = 8,\n",
        "        tf_blocks = 2,\n",
        "    )\n",
        "    self.attend = Attention(\n",
        "      listener_hidden_size=128, \n",
        "      speller_hidden_size=128, \n",
        "      projection_size=64\n",
        "    )\n",
        "    self.speller = Speller(self.attend, \n",
        "      embedding_dim=64, \n",
        "      hidden_dim=128, \n",
        "      num_lstm_layers=2, \n",
        "      context_dim=64\n",
        "    )\n",
        "\n",
        "  def forward(self, x,lx,y=None,teacher_forcing_ratio=1):\n",
        "    # Encode speech features\n",
        "    encoder_outputs, _ = self.listener(x,lx)\n",
        "\n",
        "    # We want to compute keys and values ahead of the decoding step, as they are constant for all timesteps\n",
        "    # Set keys and values using the encoder outputs\n",
        "    self.attend.set_key_value(encoder_outputs)\n",
        "\n",
        "    # Decode text with the speller using context from the attention\n",
        "    raw_outputs, attention_plots = self.speller(y=y,teacher_forcing_ratio=teacher_forcing_ratio)\n",
        "\n",
        "    return raw_outputs, attention_plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/176 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 2/176 [06:47<8:36:09, 177.99s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 3/176 [06:48<4:40:50, 97.40s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 4/176 [06:49<2:50:06, 59.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|▎         | 5/176 [06:52<1:50:16, 38.69s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|▎         | 6/176 [19:28<13:21:05, 282.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|█▉        | 35/176 [19:34<43:17, 18.42s/it]   "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 27%|██▋       | 47/176 [19:34<20:14,  9.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 34%|███▍      | 60/176 [19:34<08:56,  4.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 41%|████      | 72/176 [19:34<04:05,  2.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 47%|████▋     | 83/176 [19:35<01:55,  1.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 53%|█████▎    | 94/176 [19:35<00:51,  1.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 61%|██████    | 107/176 [19:35<00:20,  3.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 64%|██████▎   | 112/176 [19:35<00:14,  4.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 76%|███████▌  | 133/176 [19:36<00:03, 12.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 176/176 [19:36<00:00,  6.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "model = ASRModel()\n",
        "model = model.to(DEVICE)\n",
        "for batch in train_loader:\n",
        "    x, y, x_len, y_len = batch\n",
        "    output = model(x.to(DEVICE), x_len, None)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPZD3vqdUisj"
      },
      "source": [
        "# Model Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2uX2P9YVnbk",
        "outputId": "2063db54-51e0-47e9-f9b6-e36acf12540e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3302"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9LN0l5VUk_s",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ASRModel(\n",
            "  (listener): TransformerListener(\n",
            "    (base_lstm): LSTM(28, 128, batch_first=True, bidirectional=True)\n",
            "    (embedding): Sequential(\n",
            "      (0): PermuteBlock()\n",
            "      (1): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "      (2): ReLU()\n",
            "      (3): Conv1d(256, 256, kernel_size=(5,), stride=(2,), padding=(2,))\n",
            "      (4): ReLU()\n",
            "      (5): PermuteBlock()\n",
            "    )\n",
            "    (positional_encoding): PositionalEncoding()\n",
            "    (transformer_encoder): Sequential(\n",
            "      (0): TransformerEncoder(\n",
            "        (KW): Linear(in_features=256, out_features=256, bias=True)\n",
            "        (VW): Linear(in_features=256, out_features=256, bias=True)\n",
            "        (QW): Linear(in_features=256, out_features=256, bias=True)\n",
            "        (permute): PermuteBlock()\n",
            "        (attention): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (bn1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (bn2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (MLP): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (1): TransformerEncoder(\n",
            "        (KW): Linear(in_features=256, out_features=256, bias=True)\n",
            "        (VW): Linear(in_features=256, out_features=256, bias=True)\n",
            "        (QW): Linear(in_features=256, out_features=256, bias=True)\n",
            "        (permute): PermuteBlock()\n",
            "        (attention): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (bn1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (bn2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (MLP): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (attend): Attention(\n",
            "    (QW): Linear(in_features=256, out_features=128, bias=True)\n",
            "    (KW): Linear(in_features=256, out_features=128, bias=True)\n",
            "    (VW): Linear(in_features=256, out_features=128, bias=True)\n",
            "  )\n",
            "  (speller): Speller(\n",
            "    (attend): Attention(\n",
            "      (QW): Linear(in_features=256, out_features=128, bias=True)\n",
            "      (KW): Linear(in_features=256, out_features=128, bias=True)\n",
            "      (VW): Linear(in_features=256, out_features=128, bias=True)\n",
            "    )\n",
            "    (embedding): Embedding(31, 128)\n",
            "    (lstm_cells): ModuleList(\n",
            "      (0-1): 2 x LSTMCell(256, 256)\n",
            "    )\n",
            "    (output_to_char): Linear(in_features=384, out_features=128, bias=True)\n",
            "    (activation): ReLU()\n",
            "    (char_prob): Linear(in_features=128, out_features=31, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = ASRModel()\n",
        "\n",
        "model = model.to(DEVICE)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23DMfXsaU6kj"
      },
      "source": [
        "# Loss Function, Optimizers, Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "216ukmHbU-ol"
      },
      "outputs": [],
      "source": [
        "optimizer   = # TODO\n",
        "\n",
        "criterion   = # TODO\n",
        "\n",
        "scaler      = # TODO\n",
        "\n",
        "scheduler   = # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWQnB8lUVY4f"
      },
      "source": [
        "# Levenshtein Distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSsiCdxPVeZW"
      },
      "outputs": [],
      "source": [
        "# We have given you this utility function which takes a sequence of indices and converts them to a list of characters\n",
        "def indices_to_chars(indices, vocab):\n",
        "    tokens = []\n",
        "    for i in indices: # This loops through all the indices\n",
        "        if int(i) == SOS_TOKEN: # If SOS is encountered, dont add it to the final list\n",
        "            continue\n",
        "        elif int(i) == EOS_TOKEN: # If EOS is encountered, stop the decoding process\n",
        "            break\n",
        "        else:\n",
        "            tokens.append(vocab[i])\n",
        "    return tokens\n",
        "\n",
        "# To make your life more easier, we have given the Levenshtein distantce / Edit distance calculation code\n",
        "def calc_edit_distance(predictions, y, y_len, vocab= VOCAB, print_example= False):\n",
        "\n",
        "    dist                = 0\n",
        "    batch_size, seq_len = predictions.shape\n",
        "\n",
        "    for batch_idx in range(batch_size):\n",
        "\n",
        "        y_sliced    = indices_to_chars(y[batch_idx,0:y_len[batch_idx]], vocab)\n",
        "        pred_sliced = indices_to_chars(predictions[batch_idx], vocab)\n",
        "\n",
        "        # Strings - When you are using characters from the AudioDataset\n",
        "        y_string    = ''.join(y_sliced)\n",
        "        pred_string = ''.join(pred_sliced)\n",
        "\n",
        "        #dist        += Levenshtein.distance(pred_string, y_string)\n",
        "        # Comment the above abd uncomment below for toy dataset\n",
        "        dist      += Levenshtein.distance(y_sliced, pred_sliced)\n",
        "\n",
        "    if print_example:\n",
        "        # Print y_sliced and pred_sliced if you are using the toy dataset\n",
        "        print(\"\\nGround Truth : \", y_string)\n",
        "        print(\"Prediction   : \", pred_string)\n",
        "\n",
        "    dist    /= batch_size\n",
        "    return dist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu4MrSMUUIyp"
      },
      "source": [
        "# Train and Validation functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYRVKs9_2rsx"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloader, criterion, optimizer, teacher_forcing_rate):\n",
        "\n",
        "    model.train()\n",
        "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
        "\n",
        "    running_loss        = 0.0\n",
        "    running_perplexity  = 0.0\n",
        "\n",
        "    for i, (x, y, lx, ly) in enumerate(dataloader):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x, y, lx, ly = x.to(DEVICE), y.to(DEVICE), lx, ly\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "\n",
        "            raw_predictions, attention_plot = model(x, lx, y= y, tf_rate= teacher_forcing_rate)\n",
        "\n",
        "            # Predictions are of Shape (batch_size, timesteps, vocab_size).\n",
        "            # Transcripts are of shape (batch_size, timesteps) Which means that you have batch_size amount of batches with timestep number of tokens.\n",
        "            # So in total, you have batch_size*timesteps amount of characters.\n",
        "            # Similarly, in predictions, you have batch_size*timesteps amount of probability distributions.\n",
        "            # How do you need to modify transcipts and predictions so that you can calculate the CrossEntropyLoss? Hint: Use Reshape/View and read the docs\n",
        "            # Also we recommend you plot the attention weights, you should get convergence in around 10 epochs, if not, there could be something wrong with\n",
        "            # your implementation\n",
        "            loss        =  # TODO: Cross Entropy Loss\n",
        "\n",
        "            perplexity  = torch.exp(loss) # Perplexity is defined the exponential of the loss\n",
        "\n",
        "            running_loss        += loss.item()\n",
        "            running_perplexity  += perplexity.item()\n",
        "\n",
        "        # Backward on the masked loss\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # Optional: Use torch.nn.utils.clip_grad_norm to clip gradients to prevent them from exploding, if necessary\n",
        "        # If using with mixed precision, unscale the Optimizer First before doing gradient clipping\n",
        "\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            loss=\"{:.04f}\".format(running_loss/(i+1)),\n",
        "            perplexity=\"{:.04f}\".format(running_perplexity/(i+1)),\n",
        "            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])),\n",
        "            tf_rate='{:.02f}'.format(teacher_forcing_rate))\n",
        "        batch_bar.update()\n",
        "\n",
        "        del x, y, lx, ly\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    running_loss /= len(dataloader)\n",
        "    running_perplexity /= len(dataloader)\n",
        "    batch_bar.close()\n",
        "\n",
        "    return running_loss, running_perplexity, attention_plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIx3tW7a2tze"
      },
      "outputs": [],
      "source": [
        "def validate(model, dataloader):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc=\"Val\")\n",
        "\n",
        "    running_lev_dist = 0.0\n",
        "\n",
        "    for i, (x, y, lx, ly) in enumerate(dataloader):\n",
        "\n",
        "        x, y, lx, ly = x.to(DEVICE), y.to(DEVICE), lx, ly\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            raw_predictions, attentions = model(x, lx, y = None)\n",
        "\n",
        "        # Greedy Decoding\n",
        "        greedy_predictions   =  # TODO: How do you get the most likely character from each distribution in the batch?\n",
        "\n",
        "        # Calculate Levenshtein Distance\n",
        "        running_lev_dist    += calc_edit_distance(greedy_predictions, y, ly, VOCAB, print_example = False) # You can use print_example = True for one specific index i in your batches if you want\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            dist=\"{:.04f}\".format(running_lev_dist/(i+1)))\n",
        "        batch_bar.update()\n",
        "\n",
        "        del x, y, lx, ly\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close()\n",
        "    running_lev_dist /= len(dataloader)\n",
        "\n",
        "    return running_lev_dist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhwhevgWQbDX"
      },
      "source": [
        "# Wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Xbw_0eAQcoR"
      },
      "outputs": [],
      "source": [
        "# Login to Wandb\n",
        "# Initialize your Wandb Run Here\n",
        "# Save your model architecture in a txt file, and save the file to Wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vitGNx_O3MjW"
      },
      "outputs": [],
      "source": [
        "def plot_attention(attention):\n",
        "    # Function for plotting attention\n",
        "    # You need to get a diagonal plot\n",
        "    plt.clf()\n",
        "    sns.heatmap(attention, cmap='GnBu')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmZhxhNseaIr"
      },
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcTFu-AH3m4e"
      },
      "outputs": [],
      "source": [
        "best_lev_dist = float(\"inf\")\n",
        "tf_rate = 1.0\n",
        "\n",
        "for epoch in range(0, config['epochs']):\n",
        "\n",
        "    print(\"\\nEpoch: {}/{}\".format(epoch+1, config['epochs']))\n",
        "\n",
        "    # Call train and validate, get attention weights from training\n",
        "\n",
        "    # Print your metrics\n",
        "\n",
        "    # Plot Attention for a single item in the batch\n",
        "    plot_attention(attention_plot[0].cpu().detach().numpy())\n",
        "\n",
        "    # Log metrics to Wandb\n",
        "\n",
        "    # Optional: Scheduler Step / Teacher Force Schedule Step\n",
        "\n",
        "\n",
        "    if valid_dist <= best_lev_dist:\n",
        "        best_lev_dist = valid_dist\n",
        "        # Save your model checkpoint here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgFYFaBGeBqM"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndNCpcxkx2KG"
      },
      "outputs": [],
      "source": [
        "# Optional: Load your best model Checkpoint here\n",
        "\n",
        "# TODO: Create a testing function similar to validation\n",
        "# TODO: Create a file with all predictions\n",
        "# TODO: Submit to Kaggle"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "zonos",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
